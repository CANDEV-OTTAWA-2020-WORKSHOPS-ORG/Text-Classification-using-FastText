{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_fastText_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG3Y3QNaklub",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification with fastText\n",
        "\n",
        "This quick tutorial introduces the task of text classification using the [fastText](https://fasttext.cc/) library and tries to show what the full pipeline looks like from the beginning (obtaining the dataset and preparing the train/valid split) to the end (predicting labels for unseen input data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PxJCOc9kRCV",
        "colab_type": "text"
      },
      "source": [
        "Greatly based on a previously made [tutorial](https://colab.research.google.com/github/NaiveNeuron/nlp-excercises/blob/master/tutorial2-fasttext/Text_Classification_fastText.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DdFjtLj1qis",
        "colab_type": "text"
      },
      "source": [
        "## The Cooking StackExchange tags dataset\n",
        "\n",
        "We'll use a dataset of a few thousand questions asked on [Cooking StackExchange](https://cooking.stackexchange.com/) which have various tags assigned to them and which already exists in the fastText format -- basically a text file where each line contains one text document that is to be classified. Note that the lines start with `__label__` tags which denote the \"ground truth\" label for that particular text document.\n",
        "\n",
        "\n",
        "`__label__<X> __label__<Y> ... <Text>`\n",
        "\n",
        "\n",
        "For example:\n",
        "\n",
        "`__label__chocolate American equivalent for British chocolate terms`\n",
        "\n",
        "--------------------------\n",
        "\n",
        "In the next few cells we'll download the dataset and take a closer look at what the data looks like (using the [`head`](https://linux.101hacks.com/unix/head/) command) and some further statistics about the dataset (using the [`wc`](https://www.tecmint.com/wc-command-examples/) -- command).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UODjsAxt1ono",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz && tar xvzf cooking.stackexchange.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FmlmQB42tfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head cooking.stackexchange.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyPeaPL03bTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wc cooking.stackexchange.txt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IWotaq83op6",
        "colab_type": "text"
      },
      "source": [
        "We've got roughly 15k samples in our dataset. Let's split it into a training set of roughly 12k samples and testing set of 3k samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR_Yj97c3fD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 12404 cooking.stackexchange.txt > cooking.train\n",
        "!tail -n 3000 cooking.stackexchange.txt > cooking.valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgOJu8tkaAEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4QTcoLc4U1E",
        "colab_type": "text"
      },
      "source": [
        "## Installation of fastText\n",
        "\n",
        "Installing fastText is realtively easy on any Unix-like system -- running the following cell should be enough to build the `fasttext` binary, which is all we need in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAa0fruKq4Rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNZoIYhCZGAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd fastText\n",
        "!make"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFTFf8_C4SVp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp fasttext ../\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMM0RoLK6BBM",
        "colab_type": "text"
      },
      "source": [
        "## Training and testing a fastText model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMi85o9H858P",
        "colab_type": "text"
      },
      "source": [
        "The actual model fastText implements is rather simple.\n",
        "\n",
        "You can find more details on the model in section 2 the introductory paper: [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759). Note that the document representation $x_n$ is computed as the average of the embedding of all document features (words, word ngrams or character ngrams -- more on that later).\n",
        "\n",
        "\n",
        "![The actual model architecture of fastText classification](https://d3i71xaburhd42.cloudfront.net/9d6993f60539d30ee325138b3465aa020fa3bcb4/6-Figure2-1.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr70hgu38eCY",
        "colab_type": "text"
      },
      "source": [
        "In the following cell we run the `supervised` command which trains a fastText model using the data in `./cooking.train` and saves the model to `./cooking_model1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhi9ln0x4ues",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fasttext supervised -input ./cooking.train -output ./cooking_model1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4bth-HBWeruZ"
      },
      "source": [
        "### Accuracy\n",
        "\n",
        "Still, looking at just the summary statistics is not really that much fun -- that usually comes from trying the model out on some real-world data. We can easily do that with fastText by running something like the command in the following cell:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjF9yEkc-XOw",
        "colab_type": "text"
      },
      "source": [
        "Now let's see how the model does on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xEv2S4846Ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fasttext test cooking_model1.bin ./cooking.valid 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_pQoiDzbGUC",
        "colab_type": "text"
      },
      "source": [
        "Looking at the results, they do not look very stellar, as both the $P@3$ and $R@3$ can be values from 0 to 1. But what do those $P@3$ and $R@3$ actually represent?\n",
        "\n",
        "The short answer is that they are the average [precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) and [recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) averaged over all test examples. Plus, the $@3$ tell us that we only consider the top 3 most probable labels predicted by the system. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhqAOQU7iGq-",
        "colab_type": "text"
      },
      "source": [
        "**Python** version of training and testing the accuracy: https://github.com/facebookresearch/fastText/blob/master/python/doc/examples/train_supervised.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTUYOj_9bOvG",
        "colab_type": "text"
      },
      "source": [
        "### Prediction \n",
        "That probably did not help too much, so let's try to illustrate what that would mean using the following example:\n",
        "\n",
        "`__label__storage-method __label__equipment __label__bread __label__food-safety What's the purpose of a bread box?`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4HEjMV5bans",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"What's the purpose of a bread box?\" | ./fasttext predict-prob ./cooking_model1.bin - 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T6yt2RR62rH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "As we can see, our data shows that for \"*What's the purpose of a bread box?*\" the model is supposed to predict the following labels with high probability: \n",
        "\n",
        "- `storage-method`\n",
        "- `equipment`\n",
        "- `bread`\n",
        "- `food-safety`\n",
        "\n",
        "Suppose, however, that our model predicts the following labels as the top 3 most probable:\n",
        "\n",
        "- `bread`\n",
        "- `equipment`\n",
        "- `box`\n",
        "\n",
        "Since two labels (`bread` and `equipment`) out of three were found in the top 3 most probable predictions of the model, the $P@3$ in this particular example would be $P@3 = \\frac{2}{3} = 0.66 $. We can interpret that as the model *precisely predicting* $\\frac{2}{3}$ of the top 3 predictions it provided.\n",
        "\n",
        "Furthermore, since two labels (again `bread` and `equipment`) out of all 4 that were denoted in the data as correct (or relevant) could be found in the top 3 most probable predictions of the model, the $R@3$ would be $R@3 = \\frac{2}{4} = 0.5 $. That in turn can be interpreted as the model being able to *recall* $\\frac{2}{4}$ of the correct (or relevant) labels in its top 3 most probable predictions.\n",
        "\n",
        "As we can see, the reported performance and recall of our models will strongly depend on how many of the top most probable predictions should be considered. Also, we need to consider our final use case when deciding which of these to optimize for -- in the Cooking StackExchange example we may be more interested in recall than in precision.\n",
        "\n",
        "--------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPQK_NCwcyug",
        "colab_type": "text"
      },
      "source": [
        "### Arguments\n",
        "Armed with this understanding of the metrics we can try to improve, let's see what options does fastText allow us to set and see if we can get the model to perform better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfKko9217kBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fasttext supervised"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g1jFzxw__WR",
        "colab_type": "text"
      },
      "source": [
        "There are a couple of interesting options we'll dive a bit deeper into:\n",
        "\n",
        "### Character ngrams (`minx` and `maxn`)\n",
        "\n",
        "One of the interesting things fastText is capable of doing is incorporating character level information when preparing word vectors. You can find all the glory details in the [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606) paper, but the basic idea is as follows:\n",
        "\n",
        "Given the word `banana` and $n=3$, fastText would generate the following ngrams:\n",
        "\n",
        "- `<ba`\n",
        "- `ban`\n",
        "- `ana`\n",
        "- `nan`\n",
        "- `ana`\n",
        "- `na>`\n",
        "\n",
        "where the `<` and `>` represent the beginning and end of the word, respectively. That is quite useful because if we also had the word *ban* as part of the vocabulary, it would be represented as `<ban>` which makes it distinguishable from `ban` we extracted from banana.\n",
        "\n",
        "Note that we are still talking about bag of words model and thus only the presence of a respective ngram matters. Still, thanks to this nice setup we are pretty much by default able to model prefixes and suffixes. That is of huge practical value, since even if we now encountered say the word `bananoid`  which was not present in training data, thanks to the aforementioned character ngrams we are able to assign it at least some representation, rather than calling it an unknown word and replacing its occurences with `UNK`, which is what the standard approach would be.\n",
        "\n",
        "In fastText the length of ngrams can be set via the `-minn` and `-maxn` flags, which control the minimum and maximum length of ngrams fastText considers. By default these are set to 0, which basically turns this feature off.\n",
        "\n",
        "Let's see if our `bananoid` example would actually work by saving the word vectors fastText produces during training and trying to find out which words are the closest neighbors of `bananoid` in the learned vector space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlkFo6EpeDKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fasttext supervised -minn 3 -maxn 5 -input ./cooking.train -output ./cooking_model1 -saveOutput"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw6DBJi4eGhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"bananoid\" | ./fasttext nn ./cooking_model1.bin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR4-fH5GORH0",
        "colab_type": "text"
      },
      "source": [
        "Although it is obvious that our training data leave a lot to be desired, seeing `bananoid` close to `bananas`, `bananas?` and `banana?` seems to suggest that the ngrams do indeed have the potential to help with out of vocabulary words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFrjqHMZdPhi",
        "colab_type": "text"
      },
      "source": [
        "### Word ngrams\n",
        "\n",
        "Similarly to character ngrams, fastText can also generate ngrams from words in the document. This can be set using the `-wordNgrams` flag which is set to 1 by default: only unigrams (single words) are considered. When we set it to say 2, the sentece `smash all potatoes` would be represented as\n",
        "\n",
        "- `<smash>`\n",
        "- `<all>`\n",
        "- `<potatoes>`\n",
        "- `<smash all>`\n",
        "- `<all potatoes`\n",
        "\n",
        "-----------------\n",
        "\n",
        "Using these and some of the other available options, let us train a new version of the model and see how it performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFF_zi5a6P9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fasttext supervised -minCount 2 -wordNgrams 3 -minn 3 -maxn 8 -lr 0.7 -dim 100 -epoch 25 -input ./cooking.train -output cooking_model2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bSihncU6dm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fasttext test cooking_model2.bin ./cooking.valid 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bzAvHmafXvd",
        "colab_type": "text"
      },
      "source": [
        "Trying the model out on some real-world data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "godG5jDhf0tE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"Does it make sense to cook smashed potatoes?\" | ./fasttext predict-prob ./cooking_model2.bin - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gIhCAvzgK3r",
        "colab_type": "text"
      },
      "source": [
        "Alternatively we can also ask for more than just the most probable label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yl7hOlqgRLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"Does it make sense to cook smashed potatoes?\" | ./fasttext predict-prob ./cooking_model2.bin - 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2j07FbWgh0N",
        "colab_type": "text"
      },
      "source": [
        "Or ask for as many predictions as possible (`-1`) but only taking into account those that have probability higher than `0.02`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0NdM7SqgexL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"Does it make sense to cook smashed potatoes?\" | ./fasttext predict-prob ./cooking_model2.bin - -1 0.02"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knf4HLYTh-En",
        "colab_type": "text"
      },
      "source": [
        "**Python version** for predicting the labels can be found here: https://github.com/facebookresearch/fastText/tree/master/python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJODi_ubt4Y0",
        "colab_type": "text"
      },
      "source": [
        "# **Automatic hyperparameter optimization**\n",
        "\n",
        "\n",
        "As we saw in the tutorial, finding the best hyperparameters is crucial for building efficient models. However, searching the best hyperparameters manually is difficult. Parameters are dependent and the effect of each parameter vary from one dataset to another.\n",
        "\n",
        "\n",
        "FastText's autotune feature allows you to find automatically the best hyperparameters for your dataset.\n",
        "\n",
        "For more details, refer to: https://fasttext.cc/docs/en/autotune.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZQrkYq7emWe",
        "colab_type": "text"
      },
      "source": [
        "### Basic Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHbv46gI0zD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat cooking.stackexchange.txt | tr -d \"\\([;.\\!?,'/()]\\)/\" |  tr -d [:digit:] | tr \"[:upper:]\" \"[:lower:]\" > cooking.stackexchange.cleaned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcDRurnamRgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 13404 cooking.stackexchange.cleaned > cooking_at.train\n",
        "!tail -n 2000 cooking.stackexchange.cleaned > cooking_at.valid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPxPyZ2-l5GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fasttext supervised -autotune-validation cooking_at.valid -autotune-duration 600 -input cooking_at.train -output cooking_model_at\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt9EGUmerYjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./fasttext test cooking_model_at.bin ./cooking_at.valid 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTHmrpITexm-",
        "colab_type": "text"
      },
      "source": [
        "### English word vectors\n",
        "\n",
        "https://fasttext.cc/docs/en/english-vectors.html lists various pre-trained word vectors. Let's look at **wiki-news-300d-1M.vec.zip** which contains 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens). We will play around with the vectors in python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKGeGmKqXMvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.bin.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG6WMq8gX5J1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip wiki-news-300d-1M-subword.bin.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmviyMNy3JCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fasttext import load_model\n",
        "\n",
        "model = load_model('wiki-news-300d-1M-subword.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCjux5GNg-cM",
        "colab_type": "text"
      },
      "source": [
        "As we saw before, the model has not seen the word canad but can provide an embedding for it using the subword information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9faPmLcb8QTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.get_nearest_neighbors('canad')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntoVT93Mhffn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model['canad']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53Lm6mk6g9hG",
        "colab_type": "text"
      },
      "source": [
        "We can play around with word analogies. For example, we can see if our model can guess what is to France, what Berlin is to Germany.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmFVVz_JXeMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.get_analogies(\"berlin\", \"germany\", \"france\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}